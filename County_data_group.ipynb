{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elephant-xyz/notebook/blob/main/County_data_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA0ppLFpUm1j"
      },
      "source": [
        "##Welcome to step 3 of Elephant Mining\n",
        "  by reaching this step you have successfully minted your first data group (Root group), This notebook will Use your seed data To Perpare County data group and will be validated and ready for blockchain submissions.\n",
        "\n",
        "  ## What This Notebook Does:\n",
        "  This interactive notebook automates:\n",
        "\n",
        "\n",
        "-   Download properties information from County website using the Http request provided in the seed data\n",
        "-   Normalize address for each property Using [OpenAddress](https://batch.openaddresses.io/) as a source of address information\n",
        "- Converting retrieved properties data to lexicon format\n",
        "- Validating against Elephant schemas\n",
        "- Uploading to IPFS via Pinata\n",
        "- Preparing transaction data for blockchain submission\n",
        "\n",
        "## What You'll Do\n",
        "\n",
        "1. ** upload .env file** (1 minute)\n",
        "  - `.env` a file that contains your API keys a and credentials, It will be used to securely load the following environment variables\n",
        "\n",
        "\n",
        "2.  ** Download properties data** (1 minute)\n",
        "  - Upload upload-results.csv that was generated from the previous step using [Oracle notebook](https://colab.research.google.com/drive/14tSNSP8Pe-mY4VwX9JhXgfyOvzmN3kC0?usp=sharing#scrollTo=OFKp4E49651Z)\n",
        "  - Run Downloading step\n",
        "  - Input folder generated with all properties information\n",
        "3.   ** Download County Addresses from Open address ** (3 minute)\n",
        "  - Download address file from Open address\n",
        "  - Run address normalization step\n",
        "4. ** Transform Data **\n",
        "  - Enter your OpenAI Key\n",
        "  - Run AI Agent to convert the data\n",
        "  - Auto validation against [lexicon.elephant.xyz](https://lexicon.elephant.xyz/) schemas\n",
        "  - Auto Upload to IPFS and recieve content identifiers (CIDs)\n",
        "  - Generate submit-results.csv\n",
        "  - Download for oracle portal\n",
        "5. **Submit to Blockchain** (2 minutes)\n",
        "\n",
        "  - Visit oracle.elephant.xyz\n",
        "  - Upload submit-results.csv file\n",
        "  - Confirm MetaMask transactions\n",
        "\n",
        "\n",
        "Your Impact\n",
        "Each submission contributes to consensus. When three oracles submit matching data hashes, the data becomes blockchain truth and participants receive vMahout governance tokens.\n",
        "\n",
        "Let's begin.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeYZf-r-7OCU"
      },
      "source": [
        "###Create account on OpenAI\n",
        "1.   visit [OpenAI](https://platform.openai.com/)\n",
        "2.   **SingUp** or **Login** Once logged in, youâ€™ll land on the API Dashboard.\n",
        "3. Click on start building and your details then click on **Create organization**\n",
        "4. Click I'll invite my friend later\n",
        "5. Now Make your first API call Step, Name your key and project then click on **Generate API key **\n",
        "6. Copy your key then click continue\n",
        "7. select your amount of credit to purchase, you will need 15$ for AI Agent\n",
        "8. Add you payment method\n",
        "9. Paste your api key in the next step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NXggijJaldS"
      },
      "source": [
        "## Step 1: Upload .env file and upload-results.csv\n",
        "\n",
        "\n",
        "| Variable Name           | Purpose                     |\n",
        "|-------------------------|-----------------------------|\n",
        "| `OPENAI_API_KEY`        | Access to OpenAI API        |\n",
        "| `PINATA_JWT`     | Access to pinata key              |\n",
        "\n",
        "\n",
        "- Click the **folder icon** ðŸ“‚ in the left sidebar to open the file browser.\n",
        "- Then click the **\"Upload\"** button and choose your `.env` and `upload-results.csv`\n",
        "\n",
        "\n",
        "```env\n",
        "# example of .env file\n",
        "OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
        "PINATA_JWT=xxxxx\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "cDwJHYDUicuK"
      },
      "outputs": [],
      "source": [
        "# @title ## Step 2: Run to download Properties informations from County Appraiser Site\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import logging\n",
        "import csv\n",
        "import os\n",
        "import time\n",
        "from urllib.parse import urlencode\n",
        "from typing import Optional, Dict, Any\n",
        "import traceback\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PropertyDataProcessor:\n",
        "    def __init__(self, input_csv_path: str = \"upload-results.csv\", seed_csv_path: str = \"seed.csv\"):\n",
        "        self.input_csv_path = input_csv_path\n",
        "        self.seed_csv_path = seed_csv_path\n",
        "        self.ipfs_gateways = [\n",
        "            \"https://ipfs.io/ipfs/\",\n",
        "            \"https://gateway.pinata.cloud/ipfs/\",\n",
        "            \"https://cloudflare-ipfs.com/ipfs/\",\n",
        "            \"https://dweb.link/ipfs/\",\n",
        "            \"https://ipfs.infura.io/ipfs/\"\n",
        "        ]\n",
        "\n",
        "    def fetch_from_ipfs(self, cid: str) -> Optional[Dict[Any, Any]]:\n",
        "        \"\"\"Fetch data from IPFS using the provided CID with multiple gateway fallback.\"\"\"\n",
        "        for gateway in self.ipfs_gateways:\n",
        "            try:\n",
        "                url = f\"{gateway}{cid}\"\n",
        "                logger.info(f\"Trying to fetch {cid} from {gateway}\")\n",
        "                response = requests.get(url, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                return response.json()\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error fetching from {gateway}: {e}\")\n",
        "                continue\n",
        "\n",
        "        logger.error(f\"Failed to fetch data from IPFS CID {cid} from all gateways\")\n",
        "        return None\n",
        "\n",
        "    def trace_ipfs_chain(self, data_cid: str) -> Optional[Dict[Any, Any]]:\n",
        "        \"\"\"Trace through the IPFS chain to get the final property data.\"\"\"\n",
        "\n",
        "        # Step 1: Fetch the initial data using dataCid\n",
        "        logger.info(f\"Step 1: Fetching initial data from dataCid: {data_cid}\")\n",
        "        initial_data = self.fetch_from_ipfs(data_cid)\n",
        "        if not initial_data:\n",
        "            return None\n",
        "\n",
        "        # Step 2: Extract property_seed CID from relationships\n",
        "        try:\n",
        "            property_seed_cid = initial_data[\"relationships\"][\"property_seed\"][\"/\"]\n",
        "            logger.info(f\"Step 2: Found property_seed CID: {property_seed_cid}\")\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Could not find property_seed CID in initial data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 3: Fetch property_seed data\n",
        "        logger.info(f\"Step 3: Fetching property_seed data from: {property_seed_cid}\")\n",
        "        property_seed_data = self.fetch_from_ipfs(property_seed_cid)\n",
        "        if not property_seed_data:\n",
        "            return None\n",
        "\n",
        "        # Step 4: Extract \"to\" CID from property_seed data\n",
        "        try:\n",
        "            to_cid = property_seed_data[\"to\"][\"/\"]\n",
        "            logger.info(f\"Step 4: Found 'to' CID: {to_cid}\")\n",
        "        except KeyError as e:\n",
        "            logger.error(f\"Could not find 'to' CID in property_seed data: {e}\")\n",
        "            return None\n",
        "\n",
        "        # Step 5: Fetch final property data\n",
        "        logger.info(f\"Step 5: Fetching final property data from: {to_cid}\")\n",
        "        final_data = self.fetch_from_ipfs(to_cid)\n",
        "\n",
        "        return final_data\n",
        "\n",
        "    def create_seed_csv(self):\n",
        "        \"\"\"Read the input CSV, trace IPFS chain, and create seed.csv.\"\"\"\n",
        "\n",
        "        # Read the input CSV\n",
        "        try:\n",
        "            df = pd.read_csv(self.input_csv_path)\n",
        "            logger.info(f\"Loaded {len(df)} records from {self.input_csv_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error reading CSV file: {e}\")\n",
        "            return False\n",
        "\n",
        "        # Prepare output data\n",
        "        output_rows = []\n",
        "\n",
        "        for index, row in df.iterrows():\n",
        "            data_cid = row['dataCid']\n",
        "            logger.info(f\"Processing row {index + 1}: {data_cid}\")\n",
        "\n",
        "            # Trace the IPFS chain\n",
        "            final_data = self.trace_ipfs_chain(data_cid)\n",
        "\n",
        "            if final_data:\n",
        "                try:\n",
        "                    # Extract data for CSV\n",
        "                    parcel_id = final_data.get('request_identifier', '')\n",
        "                    address = final_data.get('full_address', '')\n",
        "                    county = final_data.get('county_jurisdiction', '')\n",
        "\n",
        "                    # Extract HTTP request details\n",
        "                    http_request = final_data.get('source_http_request', {})\n",
        "                    method = http_request.get('method', '')\n",
        "                    url = http_request.get('url', '')\n",
        "                    multi_value_query_string = http_request.get('multiValueQueryString', {})\n",
        "\n",
        "                    # Convert multiValueQueryString to JSON string for CSV\n",
        "                    multi_value_query_string_str = json.dumps(multi_value_query_string) if multi_value_query_string else ''\n",
        "\n",
        "                    # Create output row\n",
        "                    output_row = {\n",
        "                        'parcel_id': parcel_id,\n",
        "                        'Address': address,\n",
        "                        'method': method,\n",
        "                        'headers': '',  # Empty as per example\n",
        "                        'url': url,\n",
        "                        'multiValueQueryString': multi_value_query_string_str,\n",
        "                        'body': '',  # Empty as per example\n",
        "                        'json': '',  # Empty as per example\n",
        "                        'source_identifier': parcel_id,  # Same as parcel_id based on example\n",
        "                        'County': county\n",
        "                    }\n",
        "\n",
        "                    output_rows.append(output_row)\n",
        "                    logger.info(f\"Successfully processed parcel ID: {parcel_id}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error processing final data for row {index + 1}: {e}\")\n",
        "            else:\n",
        "                logger.error(f\"Failed to trace IPFS chain for row {index + 1}\")\n",
        "\n",
        "        # Create output DataFrame and save to CSV\n",
        "        if output_rows:\n",
        "            output_df = pd.DataFrame(output_rows)\n",
        "            output_df.to_csv(self.seed_csv_path, index=False)\n",
        "            logger.info(f\"Created {self.seed_csv_path} with {len(output_rows)} records\")\n",
        "            print(f\"Successfully created {self.seed_csv_path} with {len(output_rows)} records\")\n",
        "            return True\n",
        "        else:\n",
        "            logger.error(\"No data was successfully processed\")\n",
        "            print(\"No data was successfully processed\")\n",
        "            return False\n",
        "\n",
        "    def create_output_directory(self):\n",
        "        \"\"\"Create the input directory if it doesn't exist\"\"\"\n",
        "        if not os.path.exists('input'):\n",
        "            os.makedirs('input')\n",
        "            logger.info(\"Created 'input' directory\")\n",
        "\n",
        "    def parse_multi_value_query_string(self, query_string_json):\n",
        "        \"\"\"Parse the multiValueQueryString JSON and convert to URL parameters\"\"\"\n",
        "        try:\n",
        "            if not query_string_json or query_string_json.strip() == '':\n",
        "                return {}\n",
        "\n",
        "            query_data = json.loads(query_string_json)\n",
        "            # Convert multi-value query string to regular query parameters\n",
        "            params = {}\n",
        "            for key, values in query_data.items():\n",
        "                if isinstance(values, list) and len(values) > 0:\n",
        "                    params[key] = values[0]  # Take the first value\n",
        "                else:\n",
        "                    params[key] = values\n",
        "            return params\n",
        "        except json.JSONDecodeError as e:\n",
        "            logger.error(f\"Error parsing query string JSON: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def make_request(self, row):\n",
        "        \"\"\"Make HTTP request based on CSV row data\"\"\"\n",
        "        try:\n",
        "            parcel_id = row['parcel_id']\n",
        "            address = row['Address']\n",
        "            method = row['method'].upper()\n",
        "            url = row['url']\n",
        "            query_params = self.parse_multi_value_query_string(row['multiValueQueryString'])\n",
        "\n",
        "            logger.info(f\"Processing parcel {parcel_id} at {address}\")\n",
        "\n",
        "            # Use headers from CSV if provided, otherwise use minimal headers\n",
        "            request_headers = {}\n",
        "            if row.get('headers') and row['headers'].strip():\n",
        "                try:\n",
        "                    request_headers = json.loads(row['headers'])\n",
        "                except json.JSONDecodeError:\n",
        "                    logger.warning(f\"Invalid headers JSON for parcel {parcel_id}, using minimal headers\")\n",
        "\n",
        "            # If no headers provided or parsing failed, use minimal headers\n",
        "            if not request_headers:\n",
        "                request_headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                }\n",
        "\n",
        "            # Make the request\n",
        "            if method == 'GET':\n",
        "                response = requests.get(url, params=query_params, headers=request_headers, timeout=30)\n",
        "            elif method == 'POST':\n",
        "                # Handle POST request with body if provided\n",
        "                post_data = {}\n",
        "                if row.get('body') and row['body'].strip():\n",
        "                    try:\n",
        "                        post_data = json.loads(row['body'])\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"Invalid body JSON for parcel {parcel_id}, using empty body\")\n",
        "\n",
        "                # Handle JSON data if provided\n",
        "                if row.get('json') and row['json'].strip():\n",
        "                    try:\n",
        "                        json_data = json.loads(row['json'])\n",
        "                        response = requests.post(url, params=query_params, headers=request_headers, json=json_data, timeout=30)\n",
        "                    except json.JSONDecodeError:\n",
        "                        logger.warning(f\"Invalid JSON data for parcel {parcel_id}, using form data\")\n",
        "                        response = requests.post(url, params=query_params, headers=request_headers, data=post_data, timeout=30)\n",
        "                else:\n",
        "                    response = requests.post(url, params=query_params, headers=request_headers, data=post_data, timeout=30)\n",
        "            else:\n",
        "                logger.warning(f\"Unsupported method {method} for parcel {parcel_id}\")\n",
        "                return False\n",
        "\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Save the HTML content\n",
        "            filename = f\"input/{parcel_id}.html\"\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(response.text)\n",
        "\n",
        "            logger.info(f\"Successfully saved {filename}\")\n",
        "            return True\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            logger.error(f\"Request failed for parcel {parcel_id}: {e}\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error processing parcel {parcel_id}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def download_property_data(self):\n",
        "        \"\"\"Read seed CSV and download property data for each parcel\"\"\"\n",
        "        successful_downloads = 0\n",
        "        failed_downloads = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.seed_csv_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
        "                reader = csv.DictReader(csvfile)\n",
        "\n",
        "                # Print available columns for debugging\n",
        "                logger.info(f\"Available columns: {reader.fieldnames}\")\n",
        "\n",
        "                # Verify required columns exist\n",
        "                required_columns = ['parcel_id', 'Address', 'method', 'url', 'multiValueQueryString']\n",
        "                missing_columns = [col for col in required_columns if col not in reader.fieldnames]\n",
        "                if missing_columns:\n",
        "                    logger.error(f\"Missing required columns: {missing_columns}\")\n",
        "                    return False\n",
        "\n",
        "                logger.info(f\"Starting to process seed CSV file: {self.seed_csv_path}\")\n",
        "\n",
        "                # Convert reader to list to see total count\n",
        "                rows = list(reader)\n",
        "                total_rows = len(rows)\n",
        "                logger.info(f\"Found {total_rows} rows to process\")\n",
        "\n",
        "                for row_num, row in enumerate(rows, start=1):\n",
        "                    logger.info(f\"Processing row {row_num}/{total_rows} - Parcel: {row.get('parcel_id', 'Unknown')}\")\n",
        "\n",
        "                    try:\n",
        "                        if self.make_request(row):\n",
        "                            successful_downloads += 1\n",
        "                            logger.info(f\"âœ“ Successfully processed parcel {row.get('parcel_id')}\")\n",
        "                        else:\n",
        "                            failed_downloads += 1\n",
        "                            logger.error(f\"âœ— Failed to process parcel {row.get('parcel_id')}\")\n",
        "                    except Exception as e:\n",
        "                        failed_downloads += 1\n",
        "                        logger.error(f\"âœ— Exception processing parcel {row.get('parcel_id')}: {e}\")\n",
        "\n",
        "                    # Add a small delay to be respectful to the server\n",
        "                    time.sleep(1)\n",
        "\n",
        "            logger.info(f\"Download complete. Successful: {successful_downloads}, Failed: {failed_downloads}\")\n",
        "            return True\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            logger.error(f\"Seed CSV file '{self.seed_csv_path}' not found\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing seed CSV file: {e}\")\n",
        "            logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "            return False\n",
        "\n",
        "    def run_complete_process(self):\n",
        "        \"\"\"Run the complete process: IPFS data fetching + property download\"\"\"\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"STARTING COMPLETE PROPERTY DATA PROCESSING\")\n",
        "        logger.info(\"=\" * 60)\n",
        "\n",
        "        # Step 1: Create seed CSV from IPFS data\n",
        "        logger.info(\"STEP 1: Processing IPFS data to create seed CSV...\")\n",
        "        if not self.create_seed_csv():\n",
        "            logger.error(\"Failed to create seed CSV. Aborting.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"STEP 1 COMPLETED: Seed CSV created successfully\")\n",
        "        logger.info(\"-\" * 40)\n",
        "\n",
        "        # Step 2: Create output directory for HTML files\n",
        "        logger.info(\"STEP 2: Creating output directory...\")\n",
        "        self.create_output_directory()\n",
        "        logger.info(\"STEP 2 COMPLETED: Output directory ready\")\n",
        "        logger.info(\"-\" * 40)\n",
        "\n",
        "        # Step 3: Download property data\n",
        "        logger.info(\"STEP 3: Downloading property data from county websites...\")\n",
        "        if not self.download_property_data():\n",
        "            logger.error(\"Failed to download property data.\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"STEP 3 COMPLETED: Property data download finished\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        logger.info(\"COMPLETE PROCESS FINISHED SUCCESSFULLY\")\n",
        "        logger.info(\"=\" * 60)\n",
        "        return True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the complete property data processor\"\"\"\n",
        "\n",
        "    # Initialize the processor with default file paths\n",
        "    # You can modify these paths as needed\n",
        "    processor = PropertyDataProcessor(\n",
        "        input_csv_path=\"upload-results.csv\",  # Input CSV with dataCid column\n",
        "        seed_csv_path=\"seed.csv\"              # Output seed CSV and input for downloads\n",
        "    )\n",
        "\n",
        "    # Run the complete process\n",
        "    success = processor.run_complete_process()\n",
        "\n",
        "    if success:\n",
        "        print(\"\\nðŸŽ‰ SUCCESS: Complete property data processing finished!\")\n",
        "        print(\"- Seed CSV has been created with property request details\")\n",
        "        print(\"- Property HTML files have been downloaded to 'input/' directory\")\n",
        "    else:\n",
        "        print(\"\\nâŒ FAILED: Property data processing encountered errors\")\n",
        "        print(\"Check the logs above for detailed error information\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4FaAD_CjP0l"
      },
      "source": [
        "## Step 3: Download Open Address file for Broward County\n",
        "\n",
        "\n",
        "1. Head to [OpenAddress](https://openaddresses.io/)\n",
        "2. CLick on View download options\n",
        "3. Search for us/fl/(the county in the seed.csv) example: us/fl/broward or us/fl/palm_beach_county\n",
        "4. click on the arrow â–¶ to expand and then click on download button on the left â†“\n",
        "5. upload the file into Notebook same way you did for seed.csv\n",
        "5. wait until it is fully uploaded before moving to step 4, you can watch for the upload status in the botton left corner of the notebook\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G-R0h3PfiaYh"
      },
      "outputs": [],
      "source": [
        "# @title Step 4: Enter your OpenAddress File name (WAIT until it is fully uploaded)\n",
        "import os\n",
        "import sys\n",
        "File_Name = \"palm_beach.geojson\" # @param {\"type\":\"string\"}\n",
        "os.environ[\"OpenAddress\"] = File_Name\n",
        "def wait_for_upload(filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"âŒ File {filename} not found!\")\n",
        "        return False\n",
        "\n",
        "    print(\"Checking if upload is complete...\")\n",
        "    size1 = os.path.getsize(filename)\n",
        "    time.sleep(5)  # Wait 5 seconds\n",
        "    size2 = os.path.getsize(filename)\n",
        "\n",
        "    if size1 == size2 and size1 > 0:\n",
        "        print(f\"âœ… Upload complete! File size: {size1:,} bytes\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"âŒ File still uploading, wait longer\")\n",
        "        return False\n",
        "\n",
        "# Check the upload\n",
        "if wait_for_upload(File_Name):\n",
        "    print(\"Ready to proceed!\")\n",
        "else:\n",
        "    print(\"PLease wait for blue circle in the bottom left of the screen. If you don't see the circle, that means that you didn't upload the file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wKeTYw-0jB7g",
        "outputId": "6c1122a3-bfbb-47b3-b56c-b7d8b44baa35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSV records...\n",
            "Loaded 2 CSV records\n",
            "Loading GeoJSON from: palm_beach.geojson\n",
            "âœ… 2 addresses were successfully matched\n"
          ]
        }
      ],
      "source": [
        "# @title Step 5: Run to normalize addresse using Open Address file\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Define directional and suffix standardization\n",
        "DIRECTIONAL_PREFIXES = {\n",
        "    \"N\": \"NORTH\", \"S\": \"SOUTH\", \"E\": \"EAST\", \"W\": \"WEST\",\n",
        "    \"NE\": \"NORTHEAST\", \"NW\": \"NORTHWEST\", \"SE\": \"SOUTHEAST\", \"SW\": \"SOUTHWEST\"\n",
        "}\n",
        "\n",
        "USPS_SUFFIXES = {\n",
        "    \"ALLEE\": \"ALY\", \"ALLEY\": \"ALY\", \"ALLY\": \"ALY\", \"ALY\": \"ALY\", \"ANEX\": \"ANX\", \"ANNEX\": \"ANX\", \"ANNX\": \"ANX\", \"ANX\": \"ANX\",\n",
        "    \"ARC\": \"ARC\", \"ARCADE\": \"ARC\", \"AV\": \"AVE\", \"AVE\": \"AVE\", \"AVEN\": \"AVE\", \"AVENU\": \"AVE\", \"AVENUE\": \"AVE\", \"AVN\": \"AVE\", \"AVNUE\": \"AVE\",\n",
        "    \"BAYOO\": \"BYU\", \"BAYOU\": \"BYU\", \"BCH\": \"BCH\", \"BEACH\": \"BCH\", \"BEND\": \"BND\", \"BND\": \"BND\", \"BLF\": \"BLF\", \"BLUF\": \"BLF\", \"BLUFF\": \"BLF\", \"BLUFFS\": \"BLFS\",\n",
        "    \"BOT\": \"BTM\", \"BTM\": \"BTM\", \"BOTTM\": \"BTM\", \"BOTTOM\": \"BTM\", \"BLVD\": \"BLVD\", \"BOUL\": \"BLVD\", \"BOULEVARD\": \"BLVD\", \"BOULV\": \"BLVD\",\n",
        "    \"BR\": \"BR\", \"BRNCH\": \"BR\", \"BRANCH\": \"BR\", \"BRDGE\": \"BRG\", \"BRG\": \"BRG\", \"BRIDGE\": \"BRG\", \"BRK\": \"BRK\", \"BROOK\": \"BRK\", \"BROOKS\": \"BRKS\",\n",
        "    \"BURG\": \"BG\", \"BURGS\": \"BGS\", \"BYP\": \"BYP\", \"BYPA\": \"BYP\", \"BYPAS\": \"BYP\", \"BYPASS\": \"BYP\", \"BYPS\": \"BYP\", \"CAMP\": \"CP\", \"CP\": \"CP\", \"CMP\": \"CP\",\n",
        "    \"CANYN\": \"CYN\", \"CANYON\": \"CYN\", \"CNYN\": \"CYN\", \"CAPE\": \"CPE\", \"CPE\": \"CPE\", \"CAUSEWAY\": \"CSWY\", \"CAUSWA\": \"CSWY\", \"CSWY\": \"CSWY\",\n",
        "    \"CEN\": \"CTR\", \"CENT\": \"CTR\", \"CENTER\": \"CTR\", \"CENTR\": \"CTR\", \"CENTRE\": \"CTR\", \"CNTER\": \"CTR\", \"CNTR\": \"CTR\", \"CTR\": \"CTR\", \"CENTERS\": \"CTRS\",\n",
        "    \"CIR\": \"CIR\", \"CIRC\": \"CIR\", \"CIRCL\": \"CIR\", \"CIRCLE\": \"CIR\", \"CRCL\": \"CIR\", \"CRCLE\": \"CIR\", \"CIRCLES\": \"CIRS\", \"CLF\": \"CLF\", \"CLIFF\": \"CLF\",\n",
        "    \"CLFS\": \"CLFS\", \"CLIFFS\": \"CLFS\", \"CLB\": \"CLB\", \"CLUB\": \"CLB\", \"COMMON\": \"CMN\", \"COMMONS\": \"CMNS\", \"COR\": \"COR\", \"CORNER\": \"COR\", \"CORNERS\": \"CORS\", \"CORS\": \"CORS\",\n",
        "    \"COURSE\": \"CRSE\", \"CRSE\": \"CRSE\", \"COURT\": \"CT\", \"CT\": \"CT\", \"COURTS\": \"CTS\", \"CTS\": \"CTS\", \"COVE\": \"CV\", \"CV\": \"CV\", \"COVES\": \"CVS\",\n",
        "    \"CREEK\": \"CRK\", \"CRK\": \"CRK\", \"CRESCENT\": \"CRES\", \"CRES\": \"CRES\", \"CRSENT\": \"CRES\", \"CRSNT\": \"CRES\", \"CREST\": \"CRST\", \"CROSSING\": \"XING\", \"CRSSNG\": \"XING\", \"XING\": \"XING\",\n",
        "    \"CROSSROAD\": \"XRD\", \"CROSSROADS\": \"XRDS\", \"CURVE\": \"CURV\", \"DALE\": \"DL\", \"DL\": \"DL\", \"DAM\": \"DM\", \"DM\": \"DM\", \"DIV\": \"DV\", \"DIVIDE\": \"DV\", \"DV\": \"DV\", \"DVD\": \"DV\",\n",
        "    \"DR\": \"DR\", \"DRIV\": \"DR\", \"DRIVE\": \"DR\", \"DRV\": \"DR\", \"DRIVES\": \"DRS\", \"EST\": \"EST\", \"ESTATE\": \"EST\", \"ESTATES\": \"ESTS\", \"ESTS\": \"ESTS\",\n",
        "    \"EXP\": \"EXPY\", \"EXPR\": \"EXPY\", \"EXPRESS\": \"EXPY\", \"EXPRESSWAY\": \"EXPY\", \"EXPW\": \"EXPY\", \"EXPY\": \"EXPY\", \"EXT\": \"EXT\", \"EXTENSION\": \"EXT\", \"EXTN\": \"EXT\", \"EXTNSN\": \"EXT\", \"EXTENSIONS\": \"EXTS\", \"EXTS\": \"EXTS\",\n",
        "    \"FALL\": \"FALL\", \"FALLS\": \"FLS\", \"FLS\": \"FLS\", \"FERRY\": \"FRY\", \"FRRY\": \"FRY\", \"FRY\": \"FRY\", \"FIELD\": \"FLD\", \"FLD\": \"FLD\", \"FIELDS\": \"FLDS\", \"FLDS\": \"FLDS\",\n",
        "    \"FLAT\": \"FLT\", \"FLT\": \"FLT\", \"FLATS\": \"FLTS\", \"FLTS\": \"FLTS\", \"FORD\": \"FRD\", \"FRD\": \"FRD\", \"FORDS\": \"FRDS\", \"FOREST\": \"FRST\", \"FORESTS\": \"FRST\", \"FRST\": \"FRST\",\n",
        "    \"FORG\": \"FRG\", \"FORGE\": \"FRG\", \"FRG\": \"FRG\", \"FORGES\": \"FRGS\", \"FORK\": \"FRK\", \"FRK\": \"FRK\", \"FORKS\": \"FRKS\", \"FRKS\": \"FRKS\", \"FORT\": \"FT\", \"FRT\": \"FT\", \"FT\": \"FT\",\n",
        "    \"FREEWAY\": \"FWY\", \"FREEWY\": \"FWY\", \"FRWAY\": \"FWY\", \"FRWY\": \"FWY\", \"FWY\": \"FWY\", \"GARDEN\": \"GDN\", \"GARDN\": \"GDN\", \"GRDEN\": \"GDN\", \"GRDN\": \"GDN\", \"GARDENS\": \"GDNS\", \"GDNS\": \"GDNS\", \"GRDNS\": \"GDNS\",\n",
        "    \"GATEWAY\": \"GTWY\", \"GATEWY\": \"GTWY\", \"GATWAY\": \"GTWY\", \"GTWAY\": \"GTWY\", \"GTWY\": \"GTWY\", \"GLEN\": \"GLN\", \"GLN\": \"GLN\", \"GLENS\": \"GLNS\", \"GREEN\": \"GRN\", \"GRN\": \"GRN\", \"GREENS\": \"GRNS\",\n",
        "    \"GROV\": \"GRV\", \"GROVE\": \"GRV\", \"GRV\": \"GRV\", \"GROVES\": \"GRVS\", \"HARB\": \"HBR\", \"HARBOR\": \"HBR\", \"HARBR\": \"HBR\", \"HBR\": \"HBR\", \"HRBOR\": \"HBR\", \"HARBORS\": \"HBRS\",\n",
        "    \"HAVEN\": \"HVN\", \"HVN\": \"HVN\", \"HT\": \"HTS\", \"HTS\": \"HTS\", \"HIGHWAY\": \"HWY\", \"HIGHWY\": \"HWY\", \"HIWAY\": \"HWY\", \"HIWY\": \"HWY\", \"HWAY\": \"HWY\", \"HWY\": \"HWY\",\n",
        "    \"HILL\": \"HL\", \"HL\": \"HL\", \"HILLS\": \"HLS\", \"HLS\": \"HLS\", \"HLLW\": \"HOLW\", \"HOLLOW\": \"HOLW\", \"HOLLOWS\": \"HOLW\", \"HOLW\": \"HOLW\", \"HOLWS\": \"HOLW\",\n",
        "    \"INLET\": \"INLT\", \"INLT\": \"INLT\", \"IS\": \"IS\", \"ISLAND\": \"IS\", \"ISLND\": \"IS\", \"ISLANDS\": \"ISS\", \"ISLNDS\": \"ISS\", \"ISS\": \"ISS\", \"ISLE\": \"ISLE\", \"ISLES\": \"ISLE\",\n",
        "    \"JCT\": \"JCT\", \"JCTION\": \"JCT\", \"JCTN\": \"JCT\", \"JUNCTION\": \"JCT\", \"JUNCTN\": \"JCT\", \"JUNCTON\": \"JCT\", \"JCTNS\": \"JCTS\", \"JCTS\": \"JCTS\", \"JUNCTIONS\": \"JCTS\",\n",
        "    \"KEY\": \"KY\", \"KY\": \"KY\", \"KEYS\": \"KYS\", \"KYS\": \"KYS\", \"KNL\": \"KNL\", \"KNOL\": \"KNL\", \"KNOLL\": \"KNL\", \"KNLS\": \"KNLS\", \"KNOLLS\": \"KNLS\",\n",
        "    \"LK\": \"LK\", \"LAKE\": \"LK\", \"LKS\": \"LKS\", \"LAKES\": \"LKS\", \"LAND\": \"LAND\", \"LANDING\": \"LNDG\", \"LNDG\": \"LNDG\", \"LNDNG\": \"LNDG\", \"LANE\": \"LN\", \"LN\": \"LN\",\n",
        "    \"LGT\": \"LGT\", \"LIGHT\": \"LGT\", \"LIGHTS\": \"LGTS\", \"LF\": \"LF\", \"LOAF\": \"LF\", \"LCK\": \"LCK\", \"LOCK\": \"LCK\", \"LCKS\": \"LCKS\", \"LOCKS\": \"LCKS\",\n",
        "    \"LDG\": \"LDG\", \"LDGE\": \"LDG\", \"LODG\": \"LDG\", \"LODGE\": \"LDG\", \"LOOP\": \"LOOP\", \"LOOPS\": \"LOOP\", \"MALL\": \"MALL\", \"MNR\": \"MNR\", \"MANOR\": \"MNR\", \"MANORS\": \"MNRS\", \"MNRS\": \"MNRS\",\n",
        "    \"MEADOW\": \"MDW\", \"MDW\": \"MDW\", \"MDWS\": \"MDWS\", \"MEADOWS\": \"MDWS\", \"MEDOWS\": \"MDWS\", \"MEWS\": \"MEWS\", \"MILL\": \"ML\", \"MILLS\": \"MLS\", \"MISSION\": \"MSN\", \"MISSN\": \"MSN\", \"MSSN\": \"MSN\",\n",
        "    \"MOTORWAY\": \"MTWY\", \"MNT\": \"MT\", \"MT\": \"MT\", \"MOUNT\": \"MT\", \"MNTAIN\": \"MTN\", \"MNTN\": \"MTN\", \"MOUNTAIN\": \"MTN\", \"MOUNTIN\": \"MTN\", \"MTIN\": \"MTN\", \"MTN\": \"MTN\", \"MNTNS\": \"MTNS\", \"MOUNTAINS\": \"MTNS\",\n",
        "    \"NCK\": \"NCK\", \"NECK\": \"NCK\", \"ORCH\": \"ORCH\", \"ORCHARD\": \"ORCH\", \"ORCHRD\": \"ORCH\", \"OVAL\": \"OVAL\", \"OVL\": \"OVAL\", \"OVERPASS\": \"OPAS\", \"PARK\": \"PARK\", \"PRK\": \"PARK\", \"PARKS\": \"PARK\",\n",
        "    \"PARKWAY\": \"PKWY\", \"PARKWY\": \"PKWY\", \"PKWAY\": \"PKWY\", \"PKWY\": \"PKWY\", \"PKY\": \"PKWY\", \"PARKWAYS\": \"PKWY\", \"PKWYS\": \"PKWY\", \"PASS\": \"PASS\", \"PASSAGE\": \"PSGE\", \"PATH\": \"PATH\", \"PATHS\": \"PATH\",\n",
        "    \"PIKE\": \"PIKE\", \"PIKES\": \"PIKE\", \"PINE\": \"PNE\", \"PINES\": \"PNES\", \"PNES\": \"PNES\", \"PLACE\": \"PL\", \"PL\": \"PL\", \"PLAIN\": \"PLN\", \"PLN\": \"PLN\", \"PLAINS\": \"PLNS\", \"PLNS\": \"PLNS\",\n",
        "    \"PLAZA\": \"PLZ\", \"PLZ\": \"PLZ\", \"PLZA\": \"PLZ\", \"POINT\": \"PT\", \"PT\": \"PT\", \"POINTS\": \"PTS\", \"PTS\": \"PTS\", \"PORT\": \"PRT\", \"PRT\": \"PRT\", \"PORTS\": \"PRTS\", \"PRTS\": \"PRTS\",\n",
        "    \"PR\": \"PR\", \"PRAIRIE\": \"PR\", \"PRR\": \"PR\", \"RAD\": \"RADL\", \"RADIAL\": \"RADL\", \"RADIEL\": \"RADL\", \"RADL\": \"RADL\", \"RAMP\": \"RAMP\", \"RANCH\": \"RNCH\", \"RANCHES\": \"RNCH\", \"RNCH\": \"RNCH\", \"RNCHS\": \"RNCH\",\n",
        "    \"RAPID\": \"RPD\", \"RPD\": \"RPD\", \"RAPIDS\": \"RPDS\", \"RPDS\": \"RPDS\", \"REST\": \"RST\", \"RST\": \"RST\", \"RDG\": \"RDG\", \"RDGE\": \"RDG\", \"RIDGE\": \"RDG\", \"RDGS\": \"RDGS\", \"RIDGES\": \"RDGS\",\n",
        "    \"RIV\": \"RIV\", \"RIVER\": \"RIV\", \"RVR\": \"RIV\", \"RIVR\": \"RIV\", \"RD\": \"RD\", \"ROAD\": \"RD\", \"ROADS\": \"RDS\", \"RDS\": \"RDS\", \"ROUTE\": \"RTE\", \"ROW\": \"RTE\", \"RUE\": \"RUE\", \"RUN\": \"RUN\",\n",
        "    \"SHL\": \"SHL\", \"SHOAL\": \"SHL\", \"SHLS\": \"SHLS\", \"SHOALS\": \"SHLS\", \"SHOAR\": \"SHR\", \"SHORE\": \"SHR\", \"SHR\": \"SHR\", \"SHOARS\": \"SHRS\", \"SHORES\": \"SHRS\", \"SHRS\": \"SHRS\",\n",
        "    \"SKYWAY\": \"SKWY\", \"SPG\": \"SPG\", \"SPNG\": \"SPG\", \"SPRING\": \"SPG\", \"SPRNG\": \"SPG\", \"SPGS\": \"SPGS\", \"SPNGS\": \"SPGS\", \"SPRINGS\": \"SPGS\", \"SPRNGS\": \"SPGS\",\n",
        "    \"SPUR\": \"SPUR\", \"SPURS\": \"SPUR\", \"SQ\": \"SQ\", \"SQR\": \"SQ\", \"SQRE\": \"SQ\", \"SQU\": \"SQ\", \"SQUARE\": \"SQ\", \"SQRS\": \"SQS\", \"SQUARES\": \"SQS\",\n",
        "    \"STA\": \"STA\", \"STATION\": \"STA\", \"STATN\": \"STA\", \"STN\": \"STA\", \"STRA\": \"STRA\", \"STRAV\": \"STRA\", \"STRAVEN\": \"STRA\", \"STRAVENUE\": \"STRA\", \"STRAVN\": \"STRA\", \"STRVN\": \"STRA\", \"STRVNUE\": \"STRA\",\n",
        "    \"STREAM\": \"STRM\", \"STREME\": \"STRM\", \"STRM\": \"STRM\", \"STREET\": \"ST\", \"STRT\": \"ST\", \"ST\": \"ST\", \"STR\": \"ST\", \"STREETS\": \"STS\",\n",
        "    \"SMT\": \"SMT\", \"SUMIT\": \"SMT\", \"SUMITT\": \"SMT\", \"SUMMIT\": \"SMT\", \"TER\": \"TER\", \"TERR\": \"TER\", \"TERRACE\": \"TER\", \"THROUGHWAY\": \"TRWY\", \"TRACE\": \"TRCE\", \"TRACES\": \"TRCE\", \"TRCE\": \"TRCE\",\n",
        "    \"TRACK\": \"TRAK\", \"TRACKS\": \"TRAK\", \"TRAK\": \"TRAK\", \"TRK\": \"TRAK\", \"TRKS\": \"TRAK\", \"TRAFFICWAY\": \"TRFY\", \"TRAIL\": \"TRL\", \"TRAILS\": \"TRL\", \"TRL\": \"TRL\", \"TRLS\": \"TRL\",\n",
        "    \"TRAILER\": \"TRLR\", \"TRLR\": \"TRLR\", \"TRLRS\": \"TRLR\", \"TUNEL\": \"TUNL\", \"TUNL\": \"TUNL\", \"TUNLS\": \"TUNL\", \"TUNNEL\": \"TUNL\", \"TUNNELS\": \"TUNL\", \"TUNNL\": \"TUNL\",\n",
        "    \"TRNPK\": \"TPKE\", \"TURNPIKE\": \"TPKE\", \"TURNPK\": \"TPKE\", \"UNDERPASS\": \"UPAS\", \"UN\": \"UN\", \"UNION\": \"UN\", \"UNIONS\": \"UNS\", \"VALLEY\": \"VLY\", \"VALLY\": \"VLY\", \"VLLY\": \"VLY\", \"VLY\": \"VLY\",\n",
        "    \"VALLEYS\": \"VLYS\", \"VLYS\": \"VLYS\", \"VDCT\": \"VIA\", \"VIA\": \"VIA\", \"VIADCT\": \"VIA\", \"VIADUCT\": \"VIA\", \"VIEW\": \"VW\", \"VW\": \"VW\", \"VIEWS\": \"VWS\", \"VWS\": \"VWS\",\n",
        "    \"VILL\": \"VLG\", \"VILLAG\": \"VLG\", \"VILLAGE\": \"VLG\", \"VILLG\": \"VLG\", \"VILLIAGE\": \"VLG\", \"VLG\": \"VLG\", \"VILLAGES\": \"VLGS\", \"VLGS\": \"VLGS\", \"VILLE\": \"VL\", \"VL\": \"VL\",\n",
        "    \"VIS\": \"VIS\", \"VIST\": \"VIS\", \"VISTA\": \"VIS\", \"VST\": \"VIS\", \"VSTA\": \"VIS\", \"WALK\": \"WALK\", \"WALKS\": \"WALK\", \"WALL\": \"WALL\", \"WY\": \"WAY\", \"WAY\": \"WAY\", \"WAYS\": \"WAYS\",\n",
        "    \"WELL\": \"WL\", \"WELLS\": \"WLS\", \"WLS\": \"WLS\"\n",
        "}\n",
        "\n",
        "def parse_csv_address(address_string):\n",
        "    \"\"\"Parse CSV address: '1605 S US HIGHWAY 1 3E,PALM BEACH GARDENS'\"\"\"\n",
        "    # Split by comma - everything after comma is city\n",
        "    if ',' in address_string:\n",
        "        street_part, city_part = address_string.split(',', 1)\n",
        "        city = city_part.strip().lower()  # Normalize to lowercase\n",
        "    else:\n",
        "        street_part = address_string\n",
        "        city = None\n",
        "\n",
        "    # Parse street part\n",
        "    parts = street_part.strip().upper().split()\n",
        "\n",
        "    # Extract house number (first part should be number)\n",
        "    number = None\n",
        "    street_parts = []\n",
        "    unit = None\n",
        "\n",
        "    if parts and re.match(r'^\\d+[A-Z]?$', parts[0]):\n",
        "        number = parts[0]\n",
        "        remaining_parts = parts[1:]\n",
        "    else:\n",
        "        remaining_parts = parts\n",
        "\n",
        "    # Look for unit at the end (like \"3E\")\n",
        "    if remaining_parts and re.match(r'^\\d+[A-Z]$', remaining_parts[-1]):\n",
        "        unit = remaining_parts[-1]\n",
        "        street_parts = remaining_parts[:-1]\n",
        "    else:\n",
        "        street_parts = remaining_parts\n",
        "\n",
        "    return {\n",
        "        'number': number,\n",
        "        'street_parts': street_parts,\n",
        "        'unit': unit,\n",
        "        'city': city\n",
        "    }\n",
        "\n",
        "def normalize_street_name(street_parts):\n",
        "    \"\"\"Normalize street name parts for comparison\"\"\"\n",
        "    normalized = []\n",
        "    for part in street_parts:\n",
        "        if part in DIRECTIONAL_PREFIXES:\n",
        "            normalized.append(DIRECTIONAL_PREFIXES[part])\n",
        "        elif part in USPS_SUFFIXES:\n",
        "            normalized.append(USPS_SUFFIXES[part])\n",
        "        else:\n",
        "            normalized.append(part)\n",
        "    return ' '.join(normalized).lower()\n",
        "\n",
        "def similarity_score(str1, str2):\n",
        "    \"\"\"Calculate similarity score between two strings\"\"\"\n",
        "    return SequenceMatcher(None, str1.lower(), str2.lower()).ratio()\n",
        "\n",
        "def fuzzy_street_match(csv_street, geo_street, csv_unit=None, geo_unit=None, threshold=0.7):\n",
        "    \"\"\"Fuzzy match street names with exact unit matching\"\"\"\n",
        "    # Normalize both streets\n",
        "    csv_normalized = normalize_street_name(csv_street)\n",
        "    geo_normalized = normalize_street_name(geo_street.upper().split())\n",
        "\n",
        "    # Calculate street similarity\n",
        "    street_score = similarity_score(csv_normalized, geo_normalized)\n",
        "\n",
        "    # Exact unit matching only - no fuzzy matching\n",
        "    unit_match = False\n",
        "    if csv_unit and geo_unit:\n",
        "        csv_unit_clean = str(csv_unit).upper().strip()\n",
        "        geo_unit_clean = str(geo_unit).upper().strip()\n",
        "        unit_match = (csv_unit_clean == geo_unit_clean)\n",
        "    elif not csv_unit and not geo_unit:\n",
        "        # Both have no units - that's also a match\n",
        "        unit_match = True\n",
        "    elif not csv_unit or not geo_unit:\n",
        "        # One has unit, other doesn't - still allow street match\n",
        "        unit_match = None  # Neutral - don't penalize\n",
        "\n",
        "    return street_score >= threshold, street_score, unit_match\n",
        "\n",
        "def load_csv_records():\n",
        "    \"\"\"Load and parse CSV records\"\"\"\n",
        "    print(\"Loading CSV records...\")\n",
        "    csv_records = []\n",
        "\n",
        "    with open('seed.csv', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "            original_address = row['Address']\n",
        "            parsed = parse_csv_address(original_address)\n",
        "\n",
        "            record = {\n",
        "                'parcel_id': row['parcel_id'],\n",
        "                'original_address': original_address,\n",
        "                'parsed': parsed,\n",
        "                'county': row.get('County', ''),\n",
        "                'street_normalized': normalize_street_name(parsed['street_parts']) if parsed['street_parts'] else ''\n",
        "            }\n",
        "\n",
        "            csv_records.append(record)\n",
        "\n",
        "    print(f\"Loaded {len(csv_records)} CSV records\")\n",
        "    return csv_records\n",
        "\n",
        "def match_addresses_from_geojson():\n",
        "    # Load CSV records\n",
        "    csv_records = load_csv_records()\n",
        "\n",
        "    # Read GeoJSON path from env\n",
        "    geojson_path = os.environ.get(\"OpenAddress\")\n",
        "    if not geojson_path:\n",
        "        print(\"Warning: OpenAddress environment variable not set.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loading GeoJSON from: {geojson_path}\")\n",
        "\n",
        "    # Process GeoJSON and match\n",
        "    matches_by_parcel = defaultdict(list)\n",
        "    processed_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(geojson_path, 'r', encoding='utf-8') as geojsonfile:\n",
        "            for line_num, line in enumerate(geojsonfile, 1):\n",
        "\n",
        "                try:\n",
        "                    feature = json.loads(line)\n",
        "                    props = feature['properties']\n",
        "                    coords = feature['geometry']['coordinates']\n",
        "\n",
        "                    if not props.get('number') or not props.get('street'):\n",
        "                        continue\n",
        "\n",
        "                    geo_number = str(props['number']).upper()\n",
        "                    geo_street = props['street']\n",
        "                    geo_city = props.get('city', '').lower() if props.get('city') else None  # Normalize to lowercase\n",
        "                    geo_unit = props.get('unit', '') if props.get('unit') else None\n",
        "\n",
        "                    # Clean up ordinal suffixes in GeoJSON street\n",
        "                    geo_street_clean = re.sub(r'(\\d+)(th|st|nd|rd)', r'\\1', geo_street)\n",
        "                    geo_street_parts = geo_street_clean.upper().split()\n",
        "\n",
        "                    # Match against CSV records\n",
        "                    for csv_record in csv_records:\n",
        "                        csv_parsed = csv_record['parsed']\n",
        "\n",
        "                        # Check house number match (required)\n",
        "                        if csv_parsed['number'] and geo_number != csv_parsed['number']:\n",
        "                            continue\n",
        "\n",
        "                        # Check city match if both have cities - be more flexible\n",
        "                        city_score = None\n",
        "                        if csv_parsed['city'] and geo_city:\n",
        "                            # Allow fuzzy city matching with lower threshold\n",
        "                            city_match, city_score, _ = fuzzy_street_match([csv_parsed['city']], geo_city, threshold=0.6)\n",
        "                            # If strict city matching fails, try partial matching\n",
        "                            if not city_match:\n",
        "                                # Check if cities contain each other or share significant words\n",
        "                                csv_city_words = set(csv_parsed['city'].split())\n",
        "                                geo_city_words = set(geo_city.split())\n",
        "                                common_words = csv_city_words.intersection(geo_city_words)\n",
        "\n",
        "                                # If they share important words like palm, beach, gardens, jupiter\n",
        "                                if common_words or csv_parsed['city'] in geo_city or geo_city in csv_parsed['city']:\n",
        "                                    city_match = True\n",
        "                                    city_score = 0.7  # Moderate score for partial match\n",
        "                                else:\n",
        "                                    city_match = True  # Allow mismatch for now\n",
        "                                    city_score = 0.5\n",
        "\n",
        "                        # Street matching with exact unit matching\n",
        "                        street_match, street_score, unit_exact_match = fuzzy_street_match(\n",
        "                            csv_parsed['street_parts'],\n",
        "                            geo_street_clean,\n",
        "                            csv_parsed['unit'],\n",
        "                            geo_unit\n",
        "                        )\n",
        "\n",
        "                        # Filter: Only keep if street matches AND unit matches exactly (if both have units)\n",
        "                        if csv_parsed['unit'] and geo_unit:\n",
        "                            if not unit_exact_match:  # Skip if units don't match exactly\n",
        "                                continue\n",
        "\n",
        "                        if street_match:\n",
        "                            enriched = {\n",
        "                                **props,\n",
        "                                \"coordinates\": coords,\n",
        "                                \"original_csv_address\": csv_record['original_address'],\n",
        "                                \"parcel_id\": csv_record[\"parcel_id\"],\n",
        "                                \"county\": csv_record[\"county\"],\n",
        "                                \"match_scores\": {\n",
        "                                    \"street_score\": round(street_score, 3),\n",
        "                                    \"unit_exact_match\": unit_exact_match,\n",
        "                                    \"city_score\": round(city_score, 3) if city_score else None\n",
        "                                }\n",
        "                            }\n",
        "                            matches_by_parcel[csv_record['parcel_id']].append(enriched)\n",
        "\n",
        "                    processed_count += 1\n",
        "\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: GeoJSON file not found at {geojson_path}\")\n",
        "        return\n",
        "\n",
        "    # Save results\n",
        "    possible_addresses_dir = Path(\"possible_addresses\")\n",
        "    possible_addresses_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for parcel_id, matches in matches_by_parcel.items():\n",
        "        # Sort matches by street score\n",
        "        matches.sort(key=lambda x: x['match_scores']['street_score'], reverse=True)\n",
        "\n",
        "        # Filter logic: If we have unit matches, exclude empty unit matches\n",
        "        has_unit_matches = any(match.get('unit') for match in matches)\n",
        "\n",
        "        if has_unit_matches:\n",
        "            # Keep only matches with units\n",
        "            matches = [match for match in matches if match.get('unit')]\n",
        "\n",
        "        address_data = []\n",
        "        for match in matches:\n",
        "            # Format the data to match the desired output structure\n",
        "            address_data.append({\n",
        "                \"number\": str(match[\"number\"]),\n",
        "                \"street\": match[\"street\"].title(),  # Title case for street names\n",
        "                \"unit\": match.get(\"unit\", \"\"),\n",
        "                \"city\": match.get(\"city\", \"\").title() if match.get(\"city\") else \"\",\n",
        "                \"district\": match.get(\"district\", \"\"),\n",
        "                \"postcode\": match.get(\"postcode\", \"\"),\n",
        "                \"coordinates\": match[\"coordinates\"]\n",
        "            })\n",
        "\n",
        "        file_path = possible_addresses_dir / f\"{parcel_id}.json\"\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(address_data, f, indent=2)\n",
        "\n",
        "    print(f\"âœ… {len(matches_by_parcel)} addresses were successfully matched\")\n",
        "\n",
        "    # Show unmatched records\n",
        "    matched_parcel_ids = set(matches_by_parcel.keys())\n",
        "    unmatched = [record for record in csv_records if record[\"parcel_id\"] not in matched_parcel_ids]\n",
        "\n",
        "    if unmatched:\n",
        "        print(f\"âš ï¸  {len(unmatched)} addresses had no matches:\")\n",
        "        for record in unmatched:\n",
        "            print(f\"  Parcel: {record['parcel_id']} - '{record['original_address']}'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    match_addresses_from_geojson()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6siBnuKlzj2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# @title Step 7: Run to use AI Agent to convert properties info into validated Lexicon\n",
        "!uvx --from git+https://github.com/elephant-xyz/AI-Agent test-evaluator-agent >> logs/elephant-cli.log\n",
        "!npx -y @elephant-xyz/cli@latest validate-and-upload submit --output-csv submit-results.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npx -y @elephant-xyz/cli@latest validate-and-upload submit --output-csv submit-results.csv"
      ],
      "metadata": {
        "id": "7tjrAiSWZ11s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTNctB_VuCov"
      },
      "source": [
        "## Step 8: Submitting Your Data to the Blockchain\n",
        "\n",
        "### Submitting Your Data\n",
        "\n",
        "After running the upload command in the notebook:\n",
        "\n",
        "1. **Download your results file**\n",
        "   - The notebook will generate `submit-results.csv`\n",
        "   - This file contains your data hashes and IPFS CIDs\n",
        "   - Download it to your computer\n",
        "\n",
        "2. **Visit the Oracle Submission Portal**\n",
        "   - Go to https://oracle.elephant.xyz/\n",
        "   - Connect your MetaMask wallet when prompted\n",
        "   - Upload your `submit-results.csv` file\n",
        "\n",
        "3. **Submit transactions**\n",
        "   - The portal will read your CSV and prepare transactions\n",
        "   - Click \"Submit to Contract\" to begin\n",
        "   - MetaMask will pop up for each data entry\n",
        "   - Confirm each transaction (small gas fee applies)\n",
        "   - Wait for confirmations between submissions\n",
        "\n",
        "Once complete, your data is permanently recorded on the blockchain. You'll receive vMahout tokens as rewards after consensus is reached (when 3 different oracles submit matching data hashes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYdyCOVCyexW",
        "outputId": "b1d843a5-8a7d-4987-8b66-26d9289b47bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'owners/': No such file or directory\n",
            "rm: cannot remove 'data/': No such file or directory\n",
            "rm: cannot remove 'scripts/': No such file or directory\n",
            "rm: cannot remove 'logs/': No such file or directory\n",
            "rm: cannot remove 'results.csv': No such file or directory\n",
            "rm: cannot remove 'submit/': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! zip -r submit.zip submit/\n",
        "!rm -r owners/ data/ scripts/ logs/ results.csv submit/ submit-results.csv elephant-cli.log fact-sheet-build.log palm_beach.geojson seed.csv submit.zip submit_errors.csv submit_warnings.csv upload-results.csv\n",
        "!rm -rf /root/.local/bin/fact-sheet\n",
        "!rm -rf fact-sheet-template/\n",
        "!rm -rf /root/.elephant-fact-sheet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}